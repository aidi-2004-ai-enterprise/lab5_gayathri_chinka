# My Lab 5 Project - From Research to Reality! ğŸš€

Hey there! I'm Gayathri and this is my Lab 5 project where I finally got to implement all the research decisions I made in Lab 4. Honestly, seeing everything come together and actually work was super satisfying!

## ğŸ¯ What This Project Is About

Remember in Lab 4 when I spent forever researching the best approaches for bankruptcy prediction? (You can check out all my Lab 4 research here: https://github.com/aidi-2004-ai-enterprise/lab4_gayathri_chinka) Well, Lab 5 is where I actually built that pipeline and tested if my research was right. Spoiler alert: it totally worked! 

I created a complete machine learning pipeline that can predict if a company will go bankrupt using financial data. And the results are way better than I expected!

## ğŸ“‚ What's In My Lab 5 Folder

Here's everything I built for this project:

### ğŸ”§ Main Files
- **`training_pipeline.py`** - My complete ML pipeline that does EVERYTHING (this took me hours to get right!)
- **`data.csv`** - Same bankruptcy dataset from Lab 4 (6,819 companies with tons of financial ratios)
- **`requirements.txt`** - All the Python packages I needed (way more than Lab 4!)

### ğŸ“Š Generated Results (The Fun Stuff!)
- **`eda_analysis.png`** - Shows my Lab 4 findings were spot on (class imbalance, outliers, etc.)
- **`feature_importance.png`** - Cool chart showing which financial ratios matter most
- **`model_evaluation.png`** - Dashboard comparing all 3 models I trained
- **`shap_analysis.png`** - SHAP plots showing what drives bankruptcy predictions

### ğŸ¤– Trained Models (Ready for Production!)
- **`Random_Forest_model.pkl`** - My winning model! (94.93% ROC-AUC)
- **`XGBoost_model.pkl`** - Super close second place (94.75% ROC-AUC)
- **`Logistic_Regression_model.pkl`** - Solid baseline model (89.85% ROC-AUC)
- **`Logistic_Regression_scaler.pkl`** - Scaler for the logistic regression

### ğŸ“‹ Data Files
- **`model_evaluation_results.csv`** - All my model performance numbers in one table
- **`selected_features.txt`** - List of the top 40 features my pipeline picked

## ğŸ‰ My Amazing Results (I'm Still Excited About These!)

### ğŸ† Model Performance
I trained 3 models like I planned in Lab 4, and wow, they all performed great:

| Model | Test ROC-AUC | Test F1 | My Take |
|-------|--------------|---------|---------|
| **Random Forest** | **94.93%** | **46.75%** | **Winner! Best at generalizing** |
| XGBoost | 94.75% | 50.75% | Almost tied for first |
| Logistic Regression | 89.85% | 33.33% | Good baseline like I expected |

### ğŸ’¡ Key Discoveries
- **Top bankruptcy predictor:** Continuous interest rate (after tax) - makes total sense!
- **SMOTE worked perfectly:** Went from 154 to 4,619 bankrupt examples 
- **Outlier treatment was crucial:** Those billion-dollar values I found in Lab 4 are now reasonable
- **My feature selection:** Narrowed down from 95 to 40 most important features

## ğŸ” Cool Things I Found Out

### Lab 4 Validation (Everything Checked Out!)
- âœ… **Class imbalance:** Still exactly 1:29 ratio (3.2% bankruptcy rate)
- âœ… **Outlier problems:** Fixed all those crazy billion-dollar values
- âœ… **Distribution stability:** PSI values all under 0.1 (no data drift)
- âœ… **Tree models rule:** Random Forest and XGBoost crushed it with outliers

### Business Insights That Actually Make Sense
The most important features for predicting bankruptcy are:
1. **Continuous interest rate** - High interest = trouble paying loans
2. **Total debt/net worth ratio** - Too much debt compared to assets  
3. **Borrowing dependency** - Relying too heavily on borrowed money

These aren't random - they're exactly what you'd expect for financial distress!

## ğŸ›  How I Built This (Technical Stuff)

### My Implementation Strategy
I followed every single decision I made in Lab 4:
- **Models:** Logistic Regression (interpretable) + Random Forest (outlier robust) + XGBoost (imbalance handling)
- **Class imbalance:** SMOTE oversampling (because 1:29 is just too extreme)
- **Outliers:** Capped at 99th percentile for those impossible values
- **Feature selection:** XGBoost importance rankings (kept it simple like professor wanted)
- **Validation:** Stratified 5-fold CV to maintain that 3.2% bankruptcy rate

### The Pipeline Process
1. **EDA:** Confirmed all my Lab 4 findings
2. **Preprocessing:** SMOTE + outlier capping + train/test split
3. **Feature Selection:** XGBoost picked the top 40 from 95 features
4. **Model Training:** RandomizedSearchCV for hyperparameter tuning
5. **Evaluation:** ROC curves, calibration plots, comprehensive metrics
6. **Interpretability:** SHAP analysis (though it was a bit finicky)
7. **Saving:** All models and results exported for deployment

## ğŸš€ How to Run My Pipeline

If you want to see this in action:

1. **Set up environment:**
   ```bash
   cd lab5_Gayathri_chinka
   python -m venv lab5_env
   lab5_env\Scripts\activate
   pip install -r requirements.txt
   ```

2. **Run the magic:**
   ```bash
   python training_pipeline.py
   ```

3. **Wait and enjoy:** Takes about 5-10 minutes but generates everything!

## ğŸ¯ What I'm Most Proud Of

### Lab 4 â†’ Lab 5 Connection
The coolest part is how perfectly my Lab 4 research (check it out: https://github.com/aidi-2004-ai-enterprise/lab4_gayathri_chinka) translated to Lab 5 implementation. Every decision I made - SMOTE for imbalance, tree models for outliers, XGBoost feature selection - it all worked exactly as expected.

### Results That Matter
- **94.93% ROC-AUC** means my model is really good at distinguishing bankrupt vs healthy companies
- **Random Forest won** - proves my Lab 4 research about tree models handling outliers was right
- **Financial logic holds up** - the most important features actually make business sense

### Professional Pipeline
This isn't just a homework assignment - it's a production-ready ML pipeline with:
- Error handling and logging
- Saved models for deployment  
- Comprehensive evaluation metrics
- Professional visualizations
- Documentation and reproducibility

## ğŸ¤” Challenges I Faced (And Overcame!)

### Technical Hiccups
- **SHAP analysis was tricky** - had some compatibility issues but pipeline continued anyway
- **Package versions** - had to fix some import errors between scikit-learn versions
- **Memory management** - processing 9,238 SMOTE samples took some patience
- **Hyperparameter tuning** - kept it simple like professor requested, but still effective

### What I Learned
- **Research matters:** Lab 4 preparation made Lab 5 implementation smooth
- **Data quality is everything:** Those outlier fixes were crucial for good results
- **Model selection insights:** Tree models really do handle messy financial data better
- **Pipeline thinking:** Breaking complex ML into clear steps makes everything manageable

## ğŸ“Š Business Impact

If this were a real company project:
- **Cost savings:** Identifying bankruptcy risk early could save millions in bad loans
- **Regulatory compliance:** Model interpretability satisfies banking regulations
- **Risk management:** 94.93% accuracy means confident business decisions
- **Scalability:** Pipeline can easily retrain on new data

## ğŸ¬ What's Next

For my video presentation, I'll focus on:
- How Lab 4 research directly drove Lab 5 success
- The business story behind the technical results
- Visual walkthrough of the key findings
- Real-world deployment considerations

## ğŸ“ Files You Should Check Out

Must-see visualizations:
- **`model_evaluation.png`** - Shows how well my models perform
- **`feature_importance.png`** - Which financial ratios matter most
- **`eda_analysis.png`** - Confirms my Lab 4 data quality findings

Key results:
- **`model_evaluation_results.csv`** - All the performance numbers
- **`training_pipeline.py`** - The complete implementation

## ğŸŒŸ Final Thoughts

This project really showed me how proper research (Lab 4) leads to successful implementation (Lab 5). Every decision I made based on data exploration and literature review actually worked in practice. 

The most satisfying part? My models can actually help identify companies at risk of bankruptcy with 94.93% accuracy. That's not just a good grade - that's real-world useful!

Plus, building this end-to-end pipeline gave me confidence in tackling complex ML projects. From messy financial data to production-ready models - I can handle the whole journey now.

Thanks for checking out my work! ğŸ¯

## ğŸ”— Related Work

**Lab 4 Research Foundation:** https://github.com/aidi-2004-ai-enterprise/lab4_gayathri_chinka  
*All the research decisions that made this Lab 5 implementation successful!*

---
**Gayathri Chinka**  
*Lab 5: Complete ML Pipeline Implementation*  
*From Research to Reality - Every Decision Validated!*